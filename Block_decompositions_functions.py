import math
import sympy
import numpy as np
import itertools
from matplotlib import pyplot as plt
import networkx as nx
from matplotlib.patches import ConnectionPatch
import matplotlib.patches as mpatches
import gudhi
from scipy.spatial.distance import cdist
from ripser import ripser
from persim import plot_diagrams
import random
import time


# checks if lists/tuples are smaller in pointwise-comparison order
def is_leq_lists(list1, list2):
    # compare componentwise
    # if an element of list1 is bigger than the corresponding element in list2, then list1 !< list2
    try:
        for i in range(len(list1)):
            if list1[i] > list2[i]:
                return False
    except IndexError:
        raise IndexError("The lists don't have the same number of entries")
    return True


# returns pointwise minimum of lists
def min_of_lists(list1, list2):
    return [min(l1, l2) for l1, l2 in zip(list1, list2)]


# returns pointwise maximum of lists
def max_of_lists(list1, list2):
    return [max(l1, l2) for l1, l2 in zip(list1, list2)]


def lex_sort(list, key=None):
    #for i in
    return True


# checks whether generator of upper-set is empty (ideal is zero) or 0 (ideal is the entire ring)
def has_empty_or_zero_generator(G, num_of_entries=False):
    decomposition = []
    # empty generating set corresponds to ideal being zero
    if len(G) == 0:
        if num_of_entries == False:
            decomposition.append(sympy.Matrix([[sympy.oo], [0]]))
        else:
            decomposition.append(sympy.Matrix([[sympy.oo for i in range(num_of_entries)], [0 for i in range(num_of_entries)]]))
    # zero generating set corresponds to ideal being generated by 1
    elif len(G) == 1:
        if set(G[0]) == {0}:
            #print(G)
            return []
            #decomposition.append(sympy.Matrix([G[0], G[0]]))
    else:
        return False
    return decomposition


# Algorithm 3.1 of https://arxiv.org/abs/1512.01831
def elementary_decomposition(G, num_of_entries=False):
    elem_decomp = []
    zero_decomp = has_empty_or_zero_generator(G, num_of_entries=num_of_entries)
    if zero_decomp:
        return zero_decomp
    n = len(G[0])
    L = [[0] for i in range(n)]
    for m in G:
        for i in range(n):
            L[i].append(m[i])
    L = [list(set(x)) for x in L]
    inner_corners = list(itertools.product(*L))
    for m in G:
        inner_corners = [x for x in inner_corners if not is_leq_lists(m, x)]
    inner_corners.sort()
    outer_corners = []
    for corner in inner_corners:
        b = []
        for i in range(n):
            if corner[i] != L[i][-1]:
                b.append(L[i][L[i].index(corner[i]) + 1] - 1)
            else:
                b.append(sympy.oo)
        outer_corners.append(b)
    for oc, ic in zip(outer_corners, inner_corners):
        elem_decomp.append(sympy.Matrix((oc, ic)))
    return elem_decomp


def organized_decomposition(G, sorting_order=False, num_of_entries=False):
    # todo add option to select monomial order and extension direction (sigma and pi)
    # if G is generated by one element, check if this is zero
    zero_decomp = has_empty_or_zero_generator(G, num_of_entries=num_of_entries)
    if zero_decomp:
        return zero_decomp
    # 10.2.2 and 10.2.3
    # create list of inner corners of elementary decomposition of L
    # 3.1.1
    n = len(G[0])
    L = [[0] for i in range(n)]
    for m in G:
        for i in range(n):
            L[i].append(m[i])
    # 3.1.2
    L = [list(set(x)) for x in L]
    # 3.1.3
    inner_corners = list(itertools.product(*L))
    # 3.1.4
    for m in G:
        inner_corners = [x for x in inner_corners if not is_leq_lists(m, x)]
    # sort inner corners (default is lexsort with 0-th, then 1-st, then 2-nd ...)
    # if no sorting order is specified, then use default
    # todo improve sorting algorithm
    if not sorting_order:
        sorting_order = tuple((i for i in range(n)))
    #print(sorting_order)
    temp_sorting_indices = []
    for i in range(n):
        temp_sorting_indices.append([corner[i] for corner in inner_corners])
    temp_sorting_indices = [temp_sorting_indices[i] for i in sorting_order]
    ind = np.lexsort(tuple(temp_sorting_indices))
    inner_corners = [inner_corners[i] for i in ind]
    #print(inner_corners)
    #inner_corners.sort()
    # 3.1.5
    outer_corners = []
    for corner in inner_corners:
        b = []
        for i in range(n):
            if corner[i] != L[i][-1]:
                b.append(L[i][L[i].index(corner[i])+1]-1)
            else:
                b.append(sympy.oo)
        outer_corners.append(b)
    # 10.2.4
    block_list = []
    # 10.2.5
    k = 1
    used_inner_corners = []
    used_outer_corners = []
    while inner_corners:
        # 10.2.5.a
        a = list(inner_corners[0])
        # 10.2.5.b, proceed by alg 9.3
        # begin 9.3
        # 9.3.1
        b_k = a.copy()
        #a = list(a)
        # 9.3.2
        # a^k in paper is a here
        H_k = []
        # index j is shifted -1 relative to the paper
        for j in range(1, k-1):
            p_jk = max_of_lists(used_inner_corners[j], a)
            if is_leq_lists(p_jk, used_outer_corners[j]):
                H_k.append(p_jk)
        G_k = G + H_k
        # 9.3.3
        for h in range(n):
            b_hk_infty = b_k.copy()
            b_hk_infty[h] = sympy.oo
            G_hk = []
            for x in G_k:
                if is_leq_lists(x, b_hk_infty):
                    G_hk.append(x)
            # 9.3.3.c and d
            if G_hk == []:
                t_star = sympy.oo
            else:
                t_star = min([m_l[h] for m_l in G_hk]) - 1
            b_k[h] = t_star
        # 9.3.4
        B = sympy.Matrix([b_k, a])
        # end 9.3
        # 10.2.5.c
        block_list.append(B)
        used_inner_corners.append(a)
        used_outer_corners.append(b_k)
        # 10.2.5.d
        duplicate_inner_corners = []
        duplicate_outer_corners = []
        for corner in inner_corners:
            if is_leq_lists(corner, b_k):
                if is_leq_lists(a, corner):
                    duplicate_inner_corners.append(corner)
                    duplicate_outer_corners.append(outer_corners[inner_corners.index(corner)])
        for corner in duplicate_inner_corners:
            inner_corners.remove(corner)
        for corner in duplicate_outer_corners:
            outer_corners.remove(corner)
        k += 1
    # 10.2.6
    return block_list


# create graph that is labeled and/or directed using networkx
# if ideals is ordered by reverse inclusion, then all vertices will have a path to final layer
# The index _3 means that this is the third iteration
def create_tree_3(filtration, label=False, directed=True, arrowstyle='-|>', color_scheme='tab20', colored=False, all_ideals=True,
                  sorting_order=False, extension_order=False, decomp_type='elementary', legend=True, return_graph=False, graph_type='overlap',
                  do_show=True):
    start = time.time()
    # todo add option for different vertex color for each ideal
    # this color should be first appearance of vertex in any ideal
    if directed:
        g = nx.DiGraph()
    else:
        g = nx.Graph()
    nr_of_decomps = len(filtration)
    # Only works if at least two distinct non-trivial decomps are present, otherwise raises Error
    if nr_of_decomps > 1:
        if filtration[0] == []:
            if filtration[1] == []:
                raise AttributeError("There appear to be two trivial filtrations in the first two elements which has not "
                                     "been accounted for in the code yet.")
            else:
                n = len(filtration[1][0])
        else:
            n = len(filtration[1][0])
    elif filtration[0] == []:
        n = 1
    else:
        n = len(filtration[0][0])
    decompositions = [eval(decomp_type + f'_decomposition({I},num_of_entries={n})') for I in filtration]
    decompindex = 0
    for decomposition in decompositions:
        for block in decomposition:
            if all_ideals:
                g.add_node((tuple(block.row(1)), decompindex), blockindex=decompindex)
            else:
                if tuple(block.row(1)) not in g.nodes():
                    g.add_node(tuple(block.row(1)), blockindex=decompindex)
                else:
                    g.add_node(tuple(block.row(1)))
        decompindex += 1
    # Preprocessing for decompositions to reduce combinatorial effects on computation time
    # Index each decomposition to consider its general form and only check if the general form is acceptable
    # Works by creating digraph where an edge corresponds to the target and the source being of compatible forms
    #typegraph = nx.DiGraph()
    #L = [[0, 1] for i in range(n)]
    #all_tuples = list(itertools.product(*L))
    #tuples_by_dimension = [[] for i in range(n + 1)]
    # print(simplices_by_dimension)
    #for simplex in all_tuples:
    #    tuples_by_dimension[sum(list(simplex))].append(simplex)
    #tupledict = {}
    #for t in all_tuples:
    #    tupledict[t] = all_tuples.index(t)
    #print(tupledict)
    #all_ones = tuple([1 for i in range(n)])
    #typegraph.add_node(tupledict[all_ones])
    #all_tuples.remove(all_ones)
    #for simplex in all_tuples:
    #    typegraph.add_node(tupledict[simplex])
    #    simplexdim = sum(list(simplex))
    #    for simplex2 in tuples_by_dimension[simplexdim + 1]:
    #        if is_leq_lists(simplex, simplex2):
    #            typegraph.add_edge(tupledict[simplex], tupledict[simplex2])
    #all_tuples.append(all_ones)
    #print(typegraph)
    k = 0
    block_lists_sorted_by_inner_corner_sum = []
    block_lists_sorted_by_outer_corner_sum = []
    startt = time.time()
    for decomp in decompositions:
        #temp_dict_inner = {}
        #temp_dict_outer = {}
        blocks_by_inner_corners_by_sum = []
        blocks_by_outer_corners_by_sum = []
        for block in decomp:
            inner_sum = sum(block.row(1))
            while len(blocks_by_inner_corners_by_sum) <= inner_sum:
                blocks_by_inner_corners_by_sum.append([])
            blocks_by_inner_corners_by_sum[inner_sum].append(block)
            outer_corner = block.row(0)
            inner_max = len(blocks_by_inner_corners_by_sum)
            outer_sum = 0
            # todo: Revamp which value is used for inner_max
            for i in outer_corner:
                if i < sympy.oo:
                    outer_sum = outer_sum + i
                else:
                    outer_sum = outer_sum + inner_max
            while len(blocks_by_outer_corners_by_sum) <= outer_sum:
                blocks_by_outer_corners_by_sum.append([])
            blocks_by_outer_corners_by_sum[outer_sum].append(block)
        block_lists_sorted_by_inner_corner_sum.append(blocks_by_inner_corners_by_sum)
        block_lists_sorted_by_outer_corner_sum.append(blocks_by_outer_corners_by_sum)
        #list_of_dicts = [temp_dict_inner, temp_dict_outer]
        #block_dicts.append(list_of_dicts)
    # print(len(block_lists_sorted_by_outer_corner_sum))
    endd = time.time()
    print(endd-startt, "Sekunden für preparation")
    while decompositions:
        if len(decompositions) == 1:
            break
        blocks_0 = decompositions[0]
        #print(f"Blockzahl ist {len(blocks_0)}")
        blocks_1 = decompositions[1]
        for block_0 in blocks_0:
            #print(len(block_lists_sorted_by_outer_corner_sum[k+1]))
            for i in range(sum(block_0.row(1)), len(block_lists_sorted_by_outer_corner_sum[k+1])):
                for block_1 in block_lists_sorted_by_outer_corner_sum[k+1][i]:
                    #print("Block = ", block_1)
            #for block_1 in blocks_1:
                    if graph_type == 'overlap':
                        if all_ideals:
                            # check if block_0 and block_1 overlap by comparing inner corners and outer corners
                            if is_leq_lists(block_0.row(1), block_1.row(0)) and is_leq_lists(block_1.row(1), block_0.row(0)):
                                g.add_edge((tuple(block_0.row(1)), k), (tuple(block_1.row(1)), k + 1))
                        else:
                            if is_leq_lists(block_0.row(1), block_1.row(0)) and is_leq_lists(block_1.row(1), block_0.row(0)):
                                g.add_edge(tuple(block_0.row(1)), tuple(block_1.row(1)))
                    if graph_type == 'inner_corners':
                        corner = block_0.row(1)
                        block = block_1
                        if all_ideals:
                            if is_leq_lists(corner, block.row(0)) and is_leq_lists(block.row(1), corner):
                                g.add_edge((tuple(corner), k), (tuple(block.row(1)), k + 1))
                        else:
                            if is_leq_lists(corner, block.row(0)) and is_leq_lists(block.row(1), corner):
                                g.add_edge(tuple(corner), tuple(block.row(1)))
        #print(g)
        decompositions.pop(0)
        k += 1
    end = time.time()
    print(end-start, "Sekunden für Graph erstellen")
    if return_graph:
        return (g)
    node_colors2 = [plt.cm.tab10(g.nodes[node]['blockindex']) for node in list(g.nodes())]
    # draw the graph
    if not directed:
        nx.draw(g, with_labels=label)
    else:
        pos = nx.spring_layout(g)
        nodes = nx.draw_networkx_nodes(g, pos, node_color=node_colors2)
        if label:
            labels = nx.draw_networkx_labels(g, pos)
        edges = nx.draw_networkx_edges(
            g,
            pos,
            arrowstyle=arrowstyle,
            arrowsize=10,
            width=1,
        )
        if legend:
            handles = [mpatches.Patch(label=f'ideal {i+1}', facecolor=plt.cm.tab10(i)) for i in range(nr_of_decomps)]
            plt.legend(handles=handles)
    if do_show:
        plt.show()


def plot_trees_with_multiple_configurations(filtration, label=True, decomp_type='elementary'):
    fig, axes = plt.subplots(2,2)
    ax = axes.flatten()
    g1 = create_tree_3(filtration, label=label, directed=True, all_ideals=False, decomp_type=decomp_type, return_graph=True,
                       graph_type='inner_corners')
    g2 = create_tree_3(filtration, label=label, directed=True, all_ideals=False, decomp_type=decomp_type, return_graph=True,
                       graph_type='overlap')
    g3 = create_tree_3(filtration, label=label, directed=True, all_ideals=True, decomp_type=decomp_type, return_graph=True,
                       graph_type='inner_corners')
    g4 = create_tree_3(filtration, label=label, directed=True, all_ideals=True, decomp_type=decomp_type, return_graph=True,
                       graph_type='overlap')
    graphs = [g1, g2, g3, g4]
    for g in graphs:
        node_colors2 = [plt.cm.tab10(g.nodes[node]['blockindex']) for node in list(g.nodes())]
        # draw the graph
        #pos = nx.planar_layout(g)
        pos = nx.circular_layout(g)
        nodes = nx.draw_networkx_nodes(g, pos, node_color=node_colors2, ax=ax[graphs.index(g)])
        if label:
            labels = nx.draw_networkx_labels(g, pos, ax=ax[graphs.index(g)])
        edges = nx.draw_networkx_edges(
            g,
            pos,
            arrowstyle='-|>',
            arrowsize=10,
            width=1,
            ax=ax[graphs.index(g)]
        )
    plt.show()


# turns networkx (di)graph into a simplextree
def turn_graph_to_simplextree(graph):
    nodes = list(graph.nodes())
    #print(len(nodes))
    edges = [list(edge) for edge in graph.edges()]
    new_edges = []
    for edge in edges:
        new_edges.append([nodes.index(edge[0]), nodes.index(edge[1])])
    simplextree = gudhi.SimplexTree()
    for edge in new_edges:
        simplextree.insert(list(edge))
    for node in nodes:
        simplextree.insert([nodes.index(node)])
    return simplextree


def get_persistence_of_tree(filtration, label=False, directed=True, all_ideals=True, decomp_type='elementary'):
    st = turn_graph_to_simplextree(create_tree_3(filtration, label=label, directed=directed, all_ideals=all_ideals, decomp_type=decomp_type, return_graph=True))
    st.compute_persistence(persistence_dim_max=1)
    print(list(st.get_simplices()))
    print(st.betti_numbers())


# removes excess elements from face ideal generator (i.e. returns only minimal elements)
def remove_excess_elements(list):
    minimal_elements = []
    G = nx.DiGraph()
    for element in list:
        G.add_node(element)
    counter = 0
    for element1 in list:
        for element2 in list:
            if element1 != element2:
                if is_leq_lists(element1, element2):
                    G.add_edge(element1, element2)
    for node in G.in_degree:
        #print(node)
        if node[1] == 0:
            minimal_elements.append(node[0])
            counter += 1
    #print(counter)
    #print(len(minimal_elements))
    return minimal_elements


def face_ideal_generators(simplextree):
    # assumes that all vertices are numbered from 0 to n-1
    list_of_simplices = [simplex[0] for simplex in list(simplextree.get_simplices())]
    n = max([max(simplex) for simplex in list_of_simplices]) + 1
    #print(f"n = {n}")
    L = [[0, 1] for i in range(n)]
    # convert list_of_simplices to usable form for our purposes
    simplices_of_complex = []
    for simplex in list_of_simplices:
        all_zeros = [0 for i in range(n)]
        for i in simplex:
            all_zeros[i] = 1
        simplices_of_complex.append(all_zeros)
    #print(simplices_of_complex)
    all_simplices = list(itertools.product(*L))
    simplices_by_dimension = [[] for i in range(n+1)]
    #print(simplices_by_dimension)
    all_simplices.remove(tuple((0 for i in range(n))))
    for simplex in all_simplices:
        simplices_by_dimension[sum(list(simplex))].append(simplex)
    #print(simplices_by_dimension)
    non_simplices = []
    # remove simplices
    st_dimension = simplextree.dimension()
    #print(f"st_dimension = {st_dimension}")
    # Full complexes are handled here: There is no generator of face ideal
    if st_dimension == n-1:
        return []
    else:
        # All minimal non-faces of a simplicial complex Delta of dimension k have dimension at most dim(Delta)+1 (i.e. cardinality k+2)
        # because for subsets of dimension k+2 (i.e. cardinality k+3) and upward one may remove any vertex and still be left with a non-face
        # of Delta as the maximal cardinality of a simplex is k+1; this only works for k <= n-2
        if st_dimension < n-2:
            for i in range(st_dimension+3, n+1):
                simplices_by_dimension[i] = []
        for simplex_list in simplices_by_dimension:
            for simplex in simplex_list:
                if list(simplex) not in simplices_of_complex:
                    non_simplices.append(simplex)
                    for i in range(sum(list(simplex))+1, n+1):
                        for simplex2 in simplices_by_dimension[i]:
                            if is_leq_lists(simplex, simplex2):
                                simplices_by_dimension[i].remove(simplex2)
    # remove excess points (as a lot of elements of non_simplices are redundant)
    return(remove_excess_elements(non_simplices))


# fills simplextree until no simplices may be added without changing 1-skeleton; those correspond to cliques in the original graph
def fill_simplextree(graph, max_dim=False, return_homology=False):
    #todo Add functionality for max_dim
    st = turn_graph_to_simplextree(graph)
    if max_dim == False:
        max_dim = len(graph.nodes())
    if return_homology:
        st.compute_persistence(persistence_dim_max=max_dim)
        return st.betti_numbers()
    return st.expansion(max_dim)


# everything from point cloud to graph
def point_cloud_to_graph(distance_matrix, return_homology=False, return_graph=False, all_ideals=True, decomp_type='elementary', label=False,
                         directed=True, do_show=True, max_filtration=12.0, max_noise=0.0, max_dimension=1000, graph_type='overlap'):
    # input: Distance matrix
    # output: Graph according to previous construction
    cplx = gudhi.RipsComplex(distance_matrix=distance_matrix, max_edge_length=max_filtration)
    st = cplx.create_simplex_tree(max_dimension=max_dimension)
    #cplx = gudhi.SimplexTree.create_from_array(distance_matrix, max_filtration=max_filtration)
    #print(list(cplx.get_simplices()))
    critical_values = list(set([simplex[1] for simplex in st.get_simplices()]))
    critical_values.sort()
    if max_noise > 0:
        new_critical_values = [0, max(critical_values)]
        for i in range(1, len(critical_values)-1):
            if critical_values[i+1] - critical_values[i] >= max_noise:
                new_critical_values.append(critical_values[i])
        critical_values = new_critical_values
    critical_values.sort()
    # print(critical_values)
    filtration = []
    temp_st = gudhi.SimplexTree()
    start = time.time()
    for cval in critical_values:
        for simplex in st.get_simplices():
            if simplex[1] <= cval:
                temp_st.insert(simplex[0])
        #start = time.time()
        filtration.append(face_ideal_generators(temp_st))
    end = time.time()
    print(end-start, "Sekunden für Filtration erstellen", len(list(temp_st.get_simplices())), temp_st.dimension())
    if return_homology:
        graph = create_tree_3(filtration, label=label, return_graph=True, all_ideals=all_ideals, decomp_type=decomp_type, directed=directed,
                              do_show=False, graph_type=graph_type)
        graph2 = graph.to_undirected()
        return fill_simplextree(graph2, return_homology=True)
    return create_tree_3(filtration, label=label, return_graph=return_graph, all_ideals=all_ideals, decomp_type=decomp_type, directed=directed,
                         do_show=do_show, graph_type=graph_type)


for i in range(1):
    start = time.time()
    r = 5
    n = 10
    epsilon = 0.1
    circlePoints = [
        (r * math.cos(theta), r * math.sin(theta))
        for theta in (math.pi * 2 * i / n for i in range(n))]
    #circlePoints = [
    #    (r * math.cos(theta) + random.uniform(-epsilon, epsilon), r * math.sin(theta) + random.uniform(-epsilon, epsilon))
    #    for theta in (math.pi*2 * i/n for i in range(n))]
    dm = cdist(circlePoints, circlePoints)
    point_cloud_to_graph(dm, max_filtration=12.0, max_noise=0.1, return_homology=False, decomp_type='elementary', graph_type='overlap')
    # point_cloud_to_graph(dm, max_filtration=12.0, max_noise=0.2)
    # X = np.array(circlePoints)
    # dgms = ripser(X, maxdim=5)['dgms']
    # fig = plt.figure(figsize=(6, 6))
    # plot_diagrams(dgms, show=True)
    end = time.time()
    print(end-start, f"Sekunden vergangen für {n} Punkte")


# points=[[1, 1], [7, 0], [4, 6], [9, 6], [0, 14], [2, 19], [9, 17]]
# dm = cdist(points, points)
# point_cloud_to_graph(dm)
# print(point_cloud_to_graph(dm, return_homology=True))


# cpx000 = []
cpx00 = [[1,1,1,1]]
cpx01 = [[1,0,1,0],[1,1,0,1],[0,1,1,1]]
cpx02 = [[0,1,0,1],[1,0,1,0]]

fwd_2 = [cpx00, cpx01, cpx02]

# fill_simplextree(create_tree_3(fwd_2, return_graph=True, decomp_type='elementary'))
# print(fill_simplextree(create_tree_3(fwd_2, return_graph=True, decomp_type='elementary'), return_homology=True))

# get_persistence_of_tree(fwd_2)
# create_tree_3(fwd_2, label=False, directed=True, all_ideals=True, decomp_type='elementary', return_graph=False)
# st = turn_graph_to_simplextree(create_tree_3(fwd_2, label=False, directed=True, all_ideals=True, decomp_type='elementary', return_graph=True))
# print(list(st.get_simplices()))
# st.compute_persistence(persistence_dim_max=1)
# print(st.betti_numbers())


